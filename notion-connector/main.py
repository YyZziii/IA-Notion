from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from notion_client import Client
import os
import requests

app = FastAPI()

# ğŸ” Auth Notion via clÃ© d'API
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
notion = Client(auth=NOTION_API_KEY)

# ğŸš€ ModÃ¨le LLM configurÃ© dynamiquement
MODEL_NAME = os.getenv("MODEL_NAME", "llama3")

# ğŸ—‚ï¸ Dictionnaire des bases disponibles (nom âœ {id, colonnes})
database_map = {}

# âœ… Structure de la requÃªte attendue
class NotionQuery(BaseModel):
    question: str

# ğŸ”§ Chargement des bases de donnÃ©es Notion au dÃ©marrage
def load_databases():
    global database_map
    print("ğŸ” Chargement des bases de donnÃ©es Notion...")

    try:
        response = notion.search(filter={"property": "object", "value": "database"})
        databases = response['results']

        for db in databases:
            title = db['title'][0]['plain_text'] if db['title'] else 'Sans titre'
            db_id = db['id']

            # Extraction des colonnes
            db_info = notion.databases.retrieve(db_id)
            columns = list(db_info['properties'].keys())

            database_map[title] = {"id": db_id, "columns": columns}

        print(f"âœ… Bases trouvÃ©es : {database_map}")

    except Exception as e:
        print(f"âŒ Erreur lors du chargement des bases : {e}")

# ğŸ”§ Fonction pour dÃ©tecter la base ciblÃ©e par la question
def find_database_from_question(question: str):
    for db_name in database_map.keys():
        if db_name.lower() in question.lower():
            print(f"ğŸ” Base trouvÃ©e par le nom âœ {db_name}")
            return database_map[db_name]

    print("âš ï¸ Aucune base explicite trouvÃ©e dans la question")
    return None

# ğŸ”§ RÃ©cupÃ©ration des donnÃ©es d'une base via son ID
def fetch_database_rows(db_id):
    try:
        notion_data = notion.databases.query(database_id=db_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur Notion : {e}")

    rows = []
    for page in notion_data["results"]:
        item = {}
        for prop, val in page["properties"].items():
            if val["type"] == "title":
                item[prop] = val["title"][0]["plain_text"] if val["title"] else ""
            elif val["type"] == "rich_text":
                item[prop] = val["rich_text"][0]["plain_text"] if val["rich_text"] else ""
            elif val["type"] == "number":
                item[prop] = val["number"]
            elif val["type"] == "select":
                item[prop] = val["select"]["name"] if val["select"] else ""
            elif val["type"] == "date":
                item[prop] = val["date"]["start"] if val["date"] else ""
            else:
                item[prop] = str(val)
        rows.append(item)
    return rows

# âœ… Chargement initial des bases
load_databases()

# âœ… Endpoint principal gÃ©nÃ©rique
@app.post("/ask-llm")
def ask_llm(query: NotionQuery):
    print(f"ğŸ‘‰ Question reÃ§ue : {query.question}")

    # Recherche d'une base explicite dans la question
    db_info = find_database_from_question(query.question)

    # Mode Mono-Base (simple) âœ base spÃ©cifique trouvÃ©e
    if db_info:
        db_id = db_info["id"]
        print(f"ğŸ“‚ Base ciblÃ©e âœ {db_info}")
        rows = fetch_database_rows(db_id)

        llm_prompt = generate_prompt(
            data=rows,
            question=query.question,
            instruction="RÃ©ponds uniquement Ã  la question ci-dessous de faÃ§on simple et prÃ©cise en franÃ§ais, sans explication. Ta rÃ©ponse doit Ãªtre une seule phrase."
        )

    # Mode Multi-Bases âœ aucune base claire, on utilise toutes les donnÃ©es
    else:
        print("ğŸ” Mode Multi-Bases activÃ©")
        all_rows = {}
        for db_name, db_data in database_map.items():
            db_rows = fetch_database_rows(db_data["id"])
            all_rows[db_name] = db_rows

        llm_prompt = generate_prompt_multi_base(
            data=all_rows,
            question=query.question,
            instruction="RÃ©ponds uniquement Ã  la question ci-dessous de faÃ§on simple et prÃ©cise en franÃ§ais."
        )

    # Envoi au modÃ¨le dynamique via Ollama API
    llm_response = requests.post("http://ollama:11434/api/generate", json={
        "model": MODEL_NAME,
        "prompt": llm_prompt,
        "stream": False
    })

    if llm_response.status_code != 200:
        raise HTTPException(status_code=500, detail="Erreur lors de l'appel au modÃ¨le LLM.")

    result = llm_response.json()
    print(f"âœ… RÃ©ponse {MODEL_NAME} âœ {result['response']}")

    return {"response": result['response']}

# ğŸ”§ GÃ©nÃ©ration de prompt Mono-Base
def generate_prompt(data, question, instruction):
    return f"""
    Tu es un assistant chargÃ© d'analyser les donnÃ©es ci-dessous et de rÃ©pondre Ã  une question spÃ©cifique.

    DonnÃ©es de la base :
    {data}

    {instruction}

    Question : "{question}"

    RÃ©ponse :
    """

# ğŸ”§ GÃ©nÃ©ration de prompt Multi-Bases
def generate_prompt_multi_base(data, question, instruction):
    return f"""
    Tu es un assistant chargÃ© d'analyser les donnÃ©es ci-dessous, provenant de plusieurs bases Notion. Tu dois rÃ©pondre Ã  la question posÃ©e en utilisant toutes les informations disponibles.

    Bases de donnÃ©es :
    {data}

    {instruction}

    Question : "{question}"

    RÃ©ponse :
    """